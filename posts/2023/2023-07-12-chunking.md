---
date:   2023-07-12
tags: chunking, dask, distributed
---
# On Chunking

## What is chunking?

Chunking is the practice of representing a large n-dimensional array as a collection of smaller n-dimensional arrays.
Think of smaller boxes packed tightly in a larger box.
Note that the dimensionality of the smaller boxes is the same as that of the larger box, even though the length in one or more directions may be one.
These n-dimensional subarrays are often referred to as chunks, though it is also common to call them blocks.

This scheme can be applied to data in memory as well as on disk.
One example of the former is the excellent [Dask](https://dask.org), and examples of the latter are the chunked storage in [Netcdf4](https://www.unidata.ucar.edu/software/netcdf/) or [Zarr](https://zarr.dev/) files.

## Why chunking?

Modern computer architectures are highly parallel.
Consequently, efficient computations also require a parallel approach.
In the case of large n-dimensional data, such as long time series of two and three-dimensional fields produced by modern climate models, that means that we need to be able to treat different parts of such large arrays in parallel.

## How to chunk

To perform the chunking, we need to decide how big the smaller boxes should be, i.e. for an n-dimensional array with shape {math}`(N_1, N_2, ..., N_n)`, we need to choose for every dimension how it should be chopped up into boxes.
It is possible to choose a sequence of arbitrary numbers that add up to the total length in one dimension.
However, more commonly, we just select a chunk size per dimension, i.e. {math}`(c_1, c_2, ..., c_n)` with uniform chunk sizes within one dimension (except for the last chunk which gets the remainder).

All this raises the question of how to choose these chunk sizes.
For this, we need to consider two aspects, namely the desired size of the chunks in terms of bytes, and the aspect ratio that should be maintained between the different dimensions.
The size in bytes is linked to the underlying storage.
We want to minimize the amount of data that needs to be moved, while at the same time enabling efficient compression.
A good starting point is 1--10 MiB for modern file systems as well as cloud storage such as object storage systems like Amazon's S3.

The aspect ratio is a bit trickier and often said to be dependent on the use case.
Indeed, knowing the data is destined for a single use case can enable some optimization, but as data handlers, we are often faced with more generic requirements and need to find a compromise.
For climate data, the two main use cases are accessing time-series of data for a single physical location, and accessing entire fields for a single point in time.
Russ Rew has written a mini-series of two entries for the Unidata blog that explore both chunking in general and this use case: [Chunking Data: Why it Matters](https://www.unidata.ucar.edu/blogs/developer/en/entry/chunking_data_why_it_matters) and [Chunking Data: Choosing Shapes](https://www.unidata.ucar.edu/blogs/developer/en/entry/chunking_data_choosing_shapes).
The advice that is widely referenced in the community boils down to the idea that we should choose chunking such, that the same number of chunks need to be loaded to realize either use case.

For a three-dimensional variable with one temporal dimension and two spatial dimensions {math}`(N_t, N_y, N_x)`, this can be achieved by considering the total number of chunks, {math}`M`, to be the fourth power of some number {math}`K`, i.e. {math}`M = K^4` and choosing chunk sizes as {math}`c_t = N_t/K^2` and {math}`c_{y,x} = N_{y,x}/K`, in other words, the number of chunks in each direction is {math}`M_t = N_t/c_t = K^2` and {math}`M_{y,x} = N_{y,x}/c_{y,x} = K`, respectively, meaning that the number of chunks that need to be read to access one whole time series is {math}`M_t = K^2`, and the number of chunks that need to be read to access one whole time slice is {math}`M_{\text{slice}} = M_y * M_x = K^2`.
Indeed, this gives us some freedom since the number of chunks necessary to access one whole time slice only depends on the product {math}`M_y * M_x` and we can introduce an arbitrary constant {math}`C` to modify the chunk sizes in the horizontal dimensions as {math}`c_y = N_y/K * C` and {math}`c_x = N_x/K * 1/C` without changing the constraints.
We can exploit {math}`C` to align the chunk shapes more closely with the overall dataset shape or to bring the chunk size in bytes closer to the desired size.

Here we follow this idea with one modification.
Russ' original article considers a single data file, in which case the length of the time dimension is given by the data stored in that file.
However, in climate data, it is common to split long time series over several files, with each file only containing a part of the whole time series.
Moreover, in big comparison projects, like the [Coupled Model Intercomparison Project (CMIP)](https://www.wcrp-climate.org/wgcm-cmip) there is no standardized approach to the splitting which rather depends on the characteristics of the individual models.
In this scenario, basing the chunk size in the time direction on the amount of data stored inside a single file can generate sub-optimal chunking.
Instead, our suggestion is to consider standard lengths of time series as the basis for chunk sizes for climate data based solely on the frequency of said data as laid out in Table {ref}`standard lengths`.

:::{list-table} Standard lengths of time dimensions for different frequencies
:name: standard lengths
:header-rows: 1
:align: center

* - Frequency
  - Standard length in years
* - monthly
  - 100
* - daily
  - 100
* - sub-daily
  - 30
:::

To further increase coherence within large intercomparison projects, we might want to standardize also spatial dimensions based on nominal resolution, but this is not considered here.

For the fourth common dimension, namely the vertical dimension either up into the atmosphere or down into the ocean, we consider that the most common use cases revolve around the use of a single layer, and recommend choosing a chunk size of one (1) in that direction.